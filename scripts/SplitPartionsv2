import org.apache.spark.sql.Row
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.Dataset
import spark.implicits._
import scala.collection.mutable.ListBuffer

case class schemasample(month: String, flag: String, mon: String, flg: String)
case class schema1(mon: String, flg: String)

val inputDF = spark.sql("show partitions default.Customer_dev_Partition")

val interDF = inputDF.withColumn("month", split(col("partition"), "/").getItem(0)).withColumn("flag", split(col("partition"), "/").getItem(1)).drop(col("partition")).withColumn("mon", split(col("month"), "=").getItem(1)).withColumn("flg", split(col("flag"), "=").getItem(1))	 .as[schemasample]
					 
val sampleList = new scala.collection.mutable.ListBuffer[schema1]()

val finalList = interDF.map(row => {
	val obj1 = schema1(row.mon, row.flg)
	obj1
	}).collect.toList
	
finalList.foreach(row => {
    
    val month = row.mon
    val flg = row.flg
    
    spark.sql(s"SET hive.exec.scratchdir=/tmp/hive/;SET hive.exec.stagingdir=/tmp/hive/;SET hive.exec.dynamic.partition = true;SET hive.exec.dynamic.partition.mode = nonstrict;INSERT OVERWRITE TABLE Customer_dev_Partition PARTITION (month, y) SELECT * from default.Customer_dev_Partition WHERE month = '$month' and y = '$flg' and job not in ('services')").show()
    
    println(s"Month $month ::: Flag $flg")
})
